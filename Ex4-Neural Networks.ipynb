{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize as opt\n",
    "from scipy import io as spio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "mat=spio.loadmat('machine-learning-ex4/ex4/ex4data1.mat')\n",
    "X,y=mat['X'],mat['y']\n",
    "#y[y==10]=0 # convert label '10' back to 0 as '10' was set to be compatible with MatLab\n",
    "\n",
    "mat=spio.loadmat('machine-learning-ex4/ex4/ex4weights.mat')\n",
    "theta1,theta2=mat['Theta1'],mat['Theta2']\n",
    "\n",
    "print(theta1.shape)\n",
    "print(theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the data (to be coded...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function and sigmoid_prime\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# define function to randomly initialize weights\n",
    "def rand_ini_wgts(input_size,output_size):\n",
    "    epsilon=0.12\n",
    "    return np.random.uniform(-epsilon,epsilon,(output_size,input_size+1))\n",
    "\n",
    "# define regularized cost and gradient functions for neural network with one hidden layer\n",
    "def cost_and_grad(theta1,theta2,X,y,alpha):\n",
    "    # forward prop and cost\n",
    "    ohe=OneHotEncoder(sparse=False) # return array, not sparse to perform vectorized operations\n",
    "    y_ohe=ohe.fit_transform(y)\n",
    "    sample_size=X.shape[0]\n",
    "    ones=np.ones((sample_size,y_ohe.shape[1]))\n",
    "    X_bias=np.insert(X,0,1,axis=1)\n",
    "    z=np.dot(X_bias,theta1.T)\n",
    "    a=sigmoid(z)\n",
    "    a_bias=np.insert(a,0,1,axis=1)\n",
    "    h=sigmoid(np.dot(a_bias,theta2.T))\n",
    "    term1=np.multiply(y_ohe,np.log(h))\n",
    "    term2=np.multiply((ones-y_ohe),np.log(1-h))\n",
    "    cost=-np.sum(np.add(term1,term2))/sample_size\\\n",
    "    +alpha/(2*sample_size)*(np.sum(np.square(theta1[:,1:]))+np.sum(np.square(theta2[:,1:])))\n",
    "    \n",
    "    # backward prop and gradient\n",
    "    delta_3=h-y_ohe\n",
    "    z_bias=np.insert(z,0,1,axis=1)\n",
    "    delta_2=(np.dot(delta_3,theta2)*sigmoid_prime(z_bias))[:,1:]\n",
    "    grad_1=np.dot(delta_2.T,X_bias)/sample_size+alpha/sample_size*(np.insert(theta1[:,1:],0,0,axis=1))\n",
    "    grad_2=np.dot(delta_3.T,a_bias)/sample_size+alpha/sample_size*(np.insert(theta2[:,1:],0,0,axis=1))\n",
    "    grad=np.hstack([grad_1.ravel(),grad_2.ravel()])\n",
    "    \n",
    "    return cost.ravel(),grad\n",
    "\n",
    "# define function to compute gradient numerically (to be coded...)\n",
    "#def numeric_grad(theta1,theta2,X,y,alpha):\n",
    "    #epsilon=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters loaded from ex4weights: 0.287629\n"
     ]
    }
   ],
   "source": [
    "# test cost function with no regularization\n",
    "alpha=0\n",
    "cost_test,grad_test=cost_and_grad(theta1,theta2,X,y,alpha)\n",
    "\n",
    "print('Cost at parameters loaded from ex4weights: {0:.6f}'.format(np.asscalar(cost_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters loaded from ex4weights: 0.383770\n"
     ]
    }
   ],
   "source": [
    "# test cost function with regularization strength of 1\n",
    "alpha=1\n",
    "cost_test,grad_test=cost_and_grad(theta1,theta2,X,y,alpha)\n",
    "\n",
    "print('Cost at parameters loaded from ex4weights: {0:.6f}'.format(np.asscalar(cost_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient at inputs of [-1 -0.5 0 0.5 1]: [ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "# test sigmoid gradient\n",
    "print('Sigmoid gradient at inputs of [-1 -0.5 0 0.5 1]: {}'.format(sigmoid_prime(np.array([-1,-0.5,0,0.5,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
